{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Part 3a: Chain of Thought (CoT)"
      ],
      "metadata": {
        "id": "-Ghge7raqFAL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Concept: Latent Reasoning"
      ],
      "metadata": {
        "id": "5o2RGbM-tgfJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Name: Vaidurya Samaga\n",
        "\n",
        "SRN: PES2UG23CS668\n",
        "\n",
        "Section:K"
      ],
      "metadata": {
        "id": "2H1ZOUzjupZj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SqwIgzYFp7O6",
        "outputId": "89fb5ecb-5572-4825-ddbf-c2b1d69ac85a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/137.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.5/137.5 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install python-dotenv --upgrade --quiet langchain langchain-groq"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "import getpass\n",
        "import os\n",
        "from langchain_groq import ChatGroq\n",
        "\n",
        "if \"GROQ_API_KEY\" not in os.environ:\n",
        "    os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"Enter your Groq API Key: \")\n",
        "\n",
        "# Using Llama3.1-8b (Small/Fast) to demonstrate logic failures\n",
        "llm = ChatGroq(model=\"llama-3.1-8b-instant\", temperature=0.0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aXdgG3L4sEcE",
        "outputId": "b2b6ad7b-1623-464f-f6a9-b8f24d471702"
      },
      "execution_count": 3,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your Groq API Key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. The Experiment: A Tricky Math Problem"
      ],
      "metadata": {
        "id": "GtgytNjUtwEK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many does he have now?\"\n",
        "\n",
        "# 1. Standard Prompt (Direct Answer)\n",
        "prompt_standard = f\"Answer this question: {question}\"\n",
        "print(\"--- STANDARD (Llama3.1-8b) ---\")\n",
        "print(llm.invoke(prompt_standard).content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GbZBDuibsNbz",
        "outputId": "04210566-6b40-428f-b9ff-31070ebc89ae"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- STANDARD (Llama3.1-8b) ---\n",
            "To find out how many tennis balls Roger has now, we need to add the initial number of tennis balls he had (5) to the number of tennis balls he bought (2 cans * 3 tennis balls per can).\n",
            "\n",
            "2 cans * 3 tennis balls per can = 6 tennis balls\n",
            "\n",
            "Now, let's add the initial number of tennis balls (5) to the number of tennis balls he bought (6):\n",
            "\n",
            "5 + 6 = 11\n",
            "\n",
            "So, Roger now has 11 tennis balls.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_cot = f\"Answer this question. Let's think step by step. {question}\"\n",
        "\n",
        "print(\"--- Chain of Thought (Llama3.1-8b) ---\")\n",
        "print(llm.invoke(prompt_cot).content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CFmnMuGCs7IS",
        "outputId": "f1a7fc55-d1ec-429d-833f-f47042654c84"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Chain of Thought (Llama3.1-8b) ---\n",
            "To find out how many tennis balls Roger has now, we need to follow these steps:\n",
            "\n",
            "1. Roger already has 5 tennis balls.\n",
            "2. He buys 2 more cans of tennis balls. Each can has 3 tennis balls, so he buys 2 x 3 = 6 more tennis balls.\n",
            "3. Now, we add the tennis balls he already had (5) to the new tennis balls he bought (6). 5 + 6 = 11\n",
            "\n",
            "So, Roger now has 11 tennis balls.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part 3b: Tree of Thoughts (ToT) & Graph of Thoughts (GoT)"
      ],
      "metadata": {
        "id": "OtDu9aZds_P-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install python-dotenv --upgrade --quiet langchain langchain-groq\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "import getpass\n",
        "import os\n",
        "from langchain_groq import ChatGroq\n",
        "\n",
        "if \"GROQ_API_KEY\" not in os.environ:\n",
        "    os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"Enter your Groq API Key: \")\n",
        "\n",
        "# Using Llama3.1-8b\n",
        "llm = ChatGroq(model=\"llama-3.1-8b-instant\", temperature=0.7) # Creativity needed\n"
      ],
      "metadata": {
        "id": "mdgHFeGKs95-"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnableParallel, RunnableLambda\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "problem = \"How can I get my 5-year-old to eat vegetables?\"\n",
        "\n",
        "# Step 1: The Branch Generator\n",
        "prompt_branch = ChatPromptTemplate.from_template(\n",
        "    \"Problem: {problem}. Give me one unique, creative solution. Solution {id}:\"\n",
        ")\n",
        "\n",
        "branches = RunnableParallel(\n",
        "    sol1=prompt_branch.partial(id=\"1\") | llm | StrOutputParser(),\n",
        "    sol2=prompt_branch.partial(id=\"2\") | llm | StrOutputParser(),\n",
        "    sol3=prompt_branch.partial(id=\"3\") | llm | StrOutputParser(),\n",
        ")\n",
        "\n",
        "# Step 2: The Judge\n",
        "prompt_judge = ChatPromptTemplate.from_template(\n",
        "    \"\"\"\n",
        "    I have three proposed solutions for: '{problem}'\n",
        "\n",
        "    1: {sol1}\n",
        "    2: {sol2}\n",
        "    3: {sol3}\n",
        "\n",
        "    Act as a Child Psychologist. Pick the most sustainable one (not bribery) and explain why.\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "# Chain: Input -> Branches -> Judge -> Output\n",
        "tot_chain = (\n",
        "    RunnableParallel(problem=RunnableLambda(lambda x: x), branches=branches)\n",
        "    | (lambda x: {**x[\"branches\"], \"problem\": x[\"problem\"]})\n",
        "    | prompt_judge\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "print(\"--- Tree of Thoughts (ToT) Result ---\")\n",
        "print(tot_chain.invoke(problem))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aBBAyTeutHj_",
        "outputId": "d20aee52-ba43-4c65-87db-092e90fed986"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Tree of Thoughts (ToT) Result ---\n",
            "As a Child Psychologist, I would recommend **Solution 3: Get Them Involved in the Process** as the most sustainable approach to encouraging a 5-year-old to eat vegetables. This approach aligns with the principles of child-centered learning and development, which prioritize autonomy, agency, and engagement.\n",
            "\n",
            "Here's why I believe this approach is the most sustainable:\n",
            "\n",
            "1. **Motivation through involvement**: When children are involved in the process of meal planning, grocery shopping, and cooking, they feel more invested in the outcome. This sense of ownership and agency can motivate them to try new foods, including vegetables.\n",
            "2. **Learning and exploration**: By involving children in the cooking process, they can learn about the different textures, smells, and tastes of vegetables. This hands-on experience can foster a sense of curiosity and exploration, making vegetables more appealing and interesting.\n",
            "3. **Development of fine motor skills**: Activities like washing, chopping, and stirring can help children develop their fine motor skills, hand-eye coordination, and dexterity.\n",
            "4. **Building self-confidence**: When children are involved in the cooking process, they feel more confident and capable in the kitchen. This confidence can translate to other areas of life, including trying new foods.\n",
            "5. **Long-term benefits**: This approach can lead to long-term benefits, such as a willingness to try new foods, a greater appreciation for healthy eating, and a more positive relationship with mealtime.\n",
            "\n",
            "In contrast, while the other solutions may be effective in the short term, they may not be sustainable in the long term. For example:\n",
            "\n",
            "* **Solution 1: \"Veggie Faces\"** may become repetitive and lose its novelty value over time.\n",
            "* **Solution 2: \"Veggie Treasure Hunt\"** may be seen as a game, but it may not address the underlying issues of food aversion or resistance to trying new foods.\n",
            "* **Solution 4: \"Make it a Game\"** may rely on external motivators, such as rewards or stickers, which may not be effective in the long term.\n",
            "* **Solution 5: \"Be a Role Model\"** is essential, but it may not be enough on its own to encourage a child to try new foods.\n",
            "\n",
            "In conclusion, **Solution 3: Get Them Involved in the Process** is the most sustainable approach to encouraging a 5-year-old to eat vegetables. By involving children in the cooking process, we can foster a sense of ownership, agency, and curiosity, leading to a more positive and sustainable relationship with healthy eating.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. The Generator (Divergence)\n",
        "prompt_draft = ChatPromptTemplate.from_template(\n",
        "    \"Write a 1-sentence movie plot about: {topic}. Genre: {genre}.\"\n",
        ")\n",
        "\n",
        "drafts = RunnableParallel(\n",
        "    draft_scifi=prompt_draft.partial(genre=\"Sci-Fi\") | llm | StrOutputParser(),\n",
        "    draft_romance=prompt_draft.partial(genre=\"Romance\") | llm | StrOutputParser(),\n",
        "    draft_horror=prompt_draft.partial(genre=\"Horror\") | llm | StrOutputParser(),\n",
        ")\n",
        "\n",
        "# 2. The Aggregator (Convergence)\n",
        "prompt_combine = ChatPromptTemplate.from_template(\n",
        "    \"\"\"\n",
        "    I have three movie ideas for the topic '{topic}':\n",
        "    1. Sci-Fi: {draft_scifi}\n",
        "    2. Romance: {draft_romance}\n",
        "    3. Horror: {draft_horror}\n",
        "\n",
        "    Your task: Create a new Mega-Movie that combines the TECHNOLOGY of Sci-Fi, the PASSION of Romance, and the FEAR of Horror.\n",
        "    Write one paragraph.\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "# 3. The Chain\n",
        "got_chain = (\n",
        "    RunnableParallel(topic=RunnableLambda(lambda x: x), drafts=drafts)\n",
        "    | (lambda x: {**x[\"drafts\"], \"topic\": x[\"topic\"]})\n",
        "    | prompt_combine\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "print(\"--- Graph of Thoughts (GoT) Result ---\")\n",
        "print(got_chain.invoke(\"Time Travel\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dxr7CwUjtQ3t",
        "outputId": "15f570e3-00a0-479c-bf83-ac43d5595e90"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Graph of Thoughts (GoT) Result ---\n",
            "Here's a mega-movie concept that combines the technology of sci-fi, the passion of romance, and the fear of horror:\n",
            "\n",
            "\"Timeless Requiem\" is a heart-pounding, mind-bending thriller that follows Dr. Emma Taylor, a brilliant physicist who has spent her entire career developing a revolutionary time-travel device. When she finally succeeds in building a functioning time machine, she finds herself transported back to the 19th century, where she meets a charming and enigmatic artist named Julian. As Emma and Julian's passion for each other grows, they must navigate the treacherous landscape of Victorian-era London, where a notorious serial killer known as the \"Shadow Weaver\" is stalking and murdering victims, leaving behind a trail of eerie supernatural occurrences that suggest a dark and ancient evil is at work. But as Emma tries to escape the clutches of the Shadow Weaver, she realizes that her time-travel device has created a catastrophic ripple effect, threatening to erase entire centuries from existence - and that Julian's fate is inextricably linked to the very fabric of time itself.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Method                     | Structure                                      | Best For                                                 | Cost / Latency |\n",
        "| -------------------------- | ---------------------------------------------- | -------------------------------------------------------- | -------------- |\n",
        "| **Simple Prompting**       | Input → Output                                 | Simple facts, direct answers, summaries                  | Low            |\n",
        "| **CoT (Chain of Thought)** | Input → Steps → Output                         | Mathematical reasoning, logic, debugging                 | Medium         |\n",
        "| **ToT (Tree of Thought)**  | Input → Multiple Branches → Selection → Output | Planning, decision making, brainstorming                 | High           |\n",
        "| **GoT (Graph of Thought)** | Input → Branch → Merge / Aggregate → Output    | Research synthesis, creative reasoning, complex analysis | Very High      |\n"
      ],
      "metadata": {
        "id": "tak57LtOuZ3s"
      }
    }
  ]
}